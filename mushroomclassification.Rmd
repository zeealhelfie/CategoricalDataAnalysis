---
title: "Mushroom Classification"
author: "Zahraa Alshalal"
date: "2023-05-10"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, message = FALSE)
```

```{r}
# Libraries
# packages
library(boot)
library(dplyr)
library(plotly)
library(tidyverse)
library(MASS)
library(DataExplorer)
library(Hmisc)
library(polycor)
library(corrplot)
library(htmlwidgets)
library(moderndive)
library(leaps)
library('IRdisplay')
library(pROC)
library(car)
library(DiagrammeR)
library(plyr)
library(caret)
library(car)
library(caTools)
library(caTools)
library(boot)
```

### 1. Problem statement and dataset description:

-   The problem is to build a classification model that can predict whether a mushroom is edible or poisonous based on its physical attributes such as cap shape, cap color, odor, and more. The data-set used for this analysis is the Mushroom Classification dataset available on the UCI Machine Learning Repository. The data-set contains 8124 observations of mushrooms, with 23 features including the class (edible or poisonous).

```{r}
mushroom = read.csv("/Users/zahraaalshalal/Desktop/spring23/math449/finalproject/mushrooms.csv", header=TRUE)
glimpse(mushroom)
```

### 2. Fitting a logistic regression model with all predictors:

-   Fitting the model on the entire dataset without splitting it into training and testing sets can lead to overfitting, where the model performs well on the training data but may not generalize well to new, unseen data. To mitigate this issue, it is advisable to follow a good practice of splitting the dataset into separate training and testing sets before fitting the model. By doing so, you can assess the model's performance on the testing set, which serves as a proxy for evaluating its ability to make accurate predictions on new, unseen data.

```{r}
# Convert 'class' to a factor
mushroom$class = as.factor(mushroom$class)

# Encode all categorical variables
for (col in names(mushroom)[2:length(names(mushroom))]) {
mushroom[, col] = as.numeric(factor(mushroom[, col]))
}

# Find one-level factors
one_level_factors = sapply(mushroom, function(col) length(unique(col)) == 1)

# Remove one-level factors
mushroom = mushroom[, !one_level_factors]

# Split the data into training and testing sets
split = sample.split(mushroom$class, SplitRatio = 0.8)
train = subset(mushroom, split == TRUE)
test = subset(mushroom, split == FALSE)

# Normalize predictor variables in both train and test sets
train[, -1] = scale(train[, -1])
test[, -1] = scale(test[, -1])
```

```{r}
# logistic regression model
model = glm(class ~ ., data = train, family = "binomial")
summary(model)

```

```{r}
# predicted class labels for the test dataset
test_predicted = as.factor(predict(model, newdata = test, type = "response") > 0.5)
levels(test_predicted) = levels(test$class)

# predicted class labels for the training dataset
train_predicted = as.factor(predict(model, newdata = train, type = "response") > 0.5)
levels(train_predicted) = levels(train$class)

# accuracy
test_accuracy = sum(test_predicted == test$class) / nrow(test)
train_accuracy = sum(train_predicted == train$class) / nrow(train)

# AIC
AIC_value = AIC(model)

# BIC
BIC_value = BIC(model)

# Print model coefficients
cat("Model Coefficients:\n")
print(coef(model))

# accuracy
cat("\nAccuracy (Test Data): ", sprintf("%.2f%%", test_accuracy * 100), "\n")
cat("Accuracy (Train Data): ", sprintf("%.2f%%", train_accuracy * 100), "\n")
# AIC and BIC values
cat("AIC: ", AIC_value, "\n")
cat("BIC: ", BIC_value, "\n")


```

-   The output you provided shows the model coefficients, accuracy of 95.63% on the test data and 96.03% on the train data, and the AIC and BIC values. The high accuracy suggests that the model is effective in classifying mushrooms as edible or poisonous based on their physical attributes. The relatively low AIC and BIC values indicate that the model provides a good fit to the data, balancing model complexity and fit.

### 3. Select the best subset of variables. Perform a diagnostic on the best model. Perform all possible inferences you can think about.

```{r}
# perform "both" stepwise selection on the training data
step.model = step(model, direction = "both", trace = FALSE)
summary(step.model)
```

-   Variance Inflation Factor (VIF) is a measure of multicollinearity among the independent variables in a regression model.

```{r}
# the VIF values for the model 
vif = vif(step.model)
# display VIF values greater than 5
vif_greater_than_5 = vif[vif > 5]
vif_greater_than_5
```
+ Predictor variables in the model exhibit high multicollinearity. Specifically, the variables "Gill spacing," "Gill size," "Stalk root," "Stalk surface above ring," and "Ring type" have VIF values greater than 5, indicating strong correlation among these variables. This high multicollinearity can affect the model's interpretability.    
     
     
### 4. Use the new model to make predictions.

```{r}
# predicted class labels for the test dataset
test_predicted = as.factor(predict(step.model, newdata = test, type = "response") > 0.5)
levels(test_predicted) = levels(test$class)

# predicted class labels for the training dataset
train_predicted = as.factor(predict(step.model, newdata = train, type = "response") > 0.5)
levels(train_predicted) = levels(train$class)

# accuracy
test_accuracy = sum(test_predicted == test$class) / nrow(test)
train_accuracy = sum(train_predicted == train$class) / nrow(train)

cat("Accuracy (Test Data): ", sprintf("%.2f%%", test_accuracy * 100), "\n")
cat("Accuracy (Train Data): ", sprintf("%.2f%%", train_accuracy * 100), "\n")

```

-   The accuracy of 95.69% for the test data and 96.03% for the training data suggest that the model is able to accurately predict the class labels for the mushrooms based on the selected features in the new model. Similar to the previous model.

### 5. Use different pi_0 as a cut-off point and create a confusion table.

```{r}
# Define a vector of pi_0 values as cut-off points
pi_0_vec = seq(0.1, 0.9, by = 0.1)

# Create an empty list to store the confusion matrices for each pi_0 value
conf_mat_list = list()

for (pi_0 in pi_0_vec) {
  predictions = predict(step.model, newdata = test, type = "response")
  pred_class = ifelse(predictions > pi_0, "p", "e")
  conf_mat = table(Predicted = pred_class, Actual = test$class)
  colnames(conf_mat) = c("Edible", "Poisonous")
  rownames(conf_mat) = c("Edible", "Poisonous")
  conf_mat_list[[as.character(pi_0)]] = conf_mat
}

# Print the confusion matrices and accuracy for each pi_0 value
for (pi_0 in pi_0_vec) {
  cat("Confusion matrix for pi_0 =", pi_0, ":\n")
  print(conf_mat_list[[as.character(pi_0)]])
  accuracy = sum(diag(conf_mat_list[[as.character(pi_0)]])) / sum(conf_mat_list[[as.character(pi_0)]])
  cat("Accuracy for pi_0 =", pi_0, ":", sprintf("%.2f%%", accuracy * 100), "\n\n")
}
```

-   These confusion matrices provide a detailed breakdown of the model's performance at different cutoff values, allowing you to analyze the trade-off between true positives and true negatives based on your specific requirements. At pi_0 = 0.5, the confusion matrix shows a balanced classification result, with a relatively equal number of edible and poisonous mushrooms correctly classified.

```{r}
# Vector of pi_0 cutoff values to try
pi0_cutoffs = seq(0.1, 0.9, by = 0.1)

# Initialize an empty vector to store accuracy values
accuracy_vec = numeric(length = length(pi0_cutoffs))

# Loop over each pi_0 cutoff value and calculate accuracy
for (i in seq_along(pi0_cutoffs)) {
  pred_class = ifelse(predictions > pi0_cutoffs[i], "p", "e")
  conf_mat = table(Predicted = pred_class, Actual = test$class)
  accuracy_vec[i] = sum(diag(conf_mat)) / sum(conf_mat)
}

# Find the index of the cutoff value with the highest accuracy
best_cutoff_index = which.max(accuracy_vec)
best_cutoff = pi0_cutoffs[best_cutoff_index]

# Print the summary and best cutoff value
cat("Summary of Cutoff Values:\n")
for (i in seq_along(pi0_cutoffs)) {
  cat("Cutoff:", pi0_cutoffs[i], "\tAccuracy:", sprintf("%.2f%%", accuracy_vec[i] * 100), "\n")
}
cat("\nBest Cutoff Value:", best_cutoff, "\n")

```

-   Among these cutoff values, the best cutoff value was found to be 0.5, which achieved an accuracy of 95.69%. This means that when using 0.5 as the cutoff point, the model correctly classified 95.69% of the mushrooms as edible or poisonous.

### 6. Perform visualization of data and models.

```{r}
# Count the number of observations for each class
class_counts = table(mushroom$class)

# Create a bar plot of the class distribution
barplot(class_counts, 
        main = "Class Distribution", 
        xlab = "Class",
        ylab = "Count",
        col = c("green", "red"),
        legend = levels(mushroom$class))

```

```{r}
gill = table(mushroom$class, mushroom$`gill.size`)
barplot(gill, beside = T, legend = rownames(mushroom$`gill.size`),
        xlab = "Gill Size", ylab = "No of observations", xpd = F,
        plot = T, col = c("red", "blue"))
```

```{r}
odor = table(mushroom$class, mushroom$odor)
barplot(odor, beside = T, legend = rownames(mushroom$odor),
        xlab = "Odor", ylab = "No of observations", xpd = F,
        plot = T, col = c("red", "blue"))
```

```{r}
spore = table(mushroom$class, mushroom$`spore.print.colo`)
barplot(gill, beside = T, legend = rownames(mushroom$`spore.print.color`),
        xlab = "Spore Print Color", ylab = "No of observations", xpd = F,
        plot = T, col = c("red", "blue"))
```

```{r}
ring = table(mushroom$class, mushroom$`ring.type`)
barplot(ring, beside = T, legend = rownames(mushroom$`ring.type`),
        xlab = "Ring Type", ylab = "No of observations", xpd = F,
        plot = T, col = c("red", "blue"))
```

```{r}
population = table(mushroom$class, mushroom$population)
barplot(population, beside = T, legend = rownames(mushroom$population),
        xlab = "Population", ylab = "No of observations", xpd = F,
        plot = T, col = c("red", "blue"))
```

```{r}
habitat = table(mushroom$class, mushroom$habitat)
barplot(habitat, beside = T, legend = rownames(mushroom$population),
        xlab = "Habitate", ylab = "No of observations", xpd = F,
        plot = T, col = c("red", "blue"))
```

-   The important features of mushrooms to identify that it is safe to eat are as follows :-\
-   Odor -- creosote, foul, musty, pungent, spicy and fishy.
-   Gill Size -- only Narrow.
-   Rings -- whether large or absent.
-   Spore print color -- only brown and not black.
-   Population -- available in several places.
-   Habitat -- grown on leaves, path and urban.

```{r}
# Plot coefficients
plot(coef(step.model), xlab = "Variable", ylab = "Coefficient", main = "Step Model Coefficients")

```

```{r}
# Obtain the residuals from the model
residuals = residuals(step.model)

# Create residual plots
par(mfrow = c(2, 2))  # Set up a 2x2 grid of plots

# Residuals vs. Fitted Values plot
plot(fitted(step.model), residuals, xlab = "Fitted Values", ylab = "Residuals",
     main = "Residuals vs. Fitted Values")

# Normal Q-Q plot
qqnorm(residuals)
qqline(residuals)

# Scale-Location plot
sqrt_abs_resid = sqrt(abs(residuals))
plot(fitted(step.model), sqrt_abs_resid, xlab = "Fitted Values", ylab = "sqrt(|Residuals|)",
     main = "Scale-Location Plot")

# Residuals vs. Leverage plot
influence = hatvalues(step.model)
plot(influence, residuals, xlab = "Leverage", ylab = "Residuals",
     main = "Residuals vs. Leverage")


```

### 7. Plot the ROC curve, find AUC, and the best cutoff point for classification.

```{r}
#AUC (Area Under the Curve), ROC (Receiver Operating Characteristic) 
# predict class probabilities for the test set
probabilities = predict(step.model, newdata = test, type = "response")

# calculate FPR and TPR for various threshold values
roc_data = pROC::roc(test$class, probabilities)

# plot the ROC curve
plot(roc_data, main = "ROC Curve")

# calculate AUC
auc = pROC::auc(roc_data)
cat(sprintf("AUC: %.2f\n", auc))

# calculate the best cutoff point
cutoff = pROC::coords(roc_data, "best", ret = "threshold")[[1]]

# Calculate predicted probabilities for the test set
test_prob = predict(model, type="response", newdata=test)

# Convert probabilities to predicted classes using the cutoff
test_pred = ifelse(test_prob > cutoff, 1, 0)

# Create confusion matrix
confusion = table(test$class, test_pred)

# Calculate sensitivity and specificity
sensitivity = confusion[2,2]/sum(confusion[2,])
specificity = confusion[1,1]/sum(confusion[1,])

# Find the pi_0 cutoff value that gives the highest accuracy
best_cutoff = pi0_cutoffs[which.max(accuracy_vec)]

cat("Best cutoff:", round(best_cutoff, 2), "\n")
cat("Sensitivity:", round(sensitivity, 2), "\n")
cat("Specificity:", round(specificity, 2), "\n")
```

-   The AUC of the model is 0.99, indicating good performance in distinguishing between edible and poisonous mushrooms.

-   The best cutoff point for classification is 0.5, which maximizes the trade-off between sensitivity and specificity. The sensitivity of the model is 0.94. This means that it correctly identifies 94% of the positive cases, while the specificity is 0.97, indicating that it correctly identifies 97% of the negative cases.

### 8. Perform LOOCV and k-fold cross-validation.

```{r}
model = glm(class ~ ., data = mushroom, family = "binomial")
step.model = step(model, direction = "both", trace = FALSE)

# Make a copy of the original dataset
mushroom_copy = mushroom

# Set the desired sample size for LOOCV
sample_size = 1000

# Randomly select a smaller sample from the dataset
sample_indices = sample(1:nrow(mushroom_copy), size = sample_size, replace = FALSE)
sample_data = mushroom_copy[sample_indices, ]


# Perform LOOCV on the smaller sample
loocv_result = cv.glm(sample_data, step.model, K = nrow(sample_data))

# k-fold Cross-Validation (k = 10)
k_fold = 10
kfold_result = cv.glm(mushroom, step.model, K = k_fold)

# Accessing the performance measures
loocv_error = 1 - loocv_result$delta[1]
kfold_error = 1 - kfold_result$delta[1]

# Print the results
cat("LOOCV Error:", loocv_error, "\n")
cat("K-fold Cross-Validation Errors:", kfold_error, "\n")
```

### 9. Try the probit link and the identity links to model data.

```{r}
# Model with probit link
probit_model = glm(class ~ ., data = train, family = binomial(link = "probit"))

# Manually specify starting values for identity model
identity_start = coef(probit_model)

# Add a small perturbation to the starting values
identity_start = identity_start + 0.01

# Encode 'class' variable as 0 and 1
train$class = as.numeric(train$class) - 1
test$class = as.numeric(test$class) - 1

# Model with identity link using starting values and Identity Links family
identity_model = glm(class ~ ., data = train, family = gaussian(link = "identity"), start = identity_start)

# Predict on the test set using probit model
probit_predictions = predict(probit_model, newdata = test, type = "response")

# Predict on the train set using probit model
probit_train_predictions = predict(probit_model, newdata = train, type = "response")

# Predict on the train set using identity model
identity_train_predictions = predict(identity_model, newdata = train, type = "response")

# Convert probabilities to class predictions for the train set
probit_train_predictions = ifelse(probit_train_predictions >= 0.5, 1, 0)
identity_train_predictions = ifelse(identity_train_predictions >= 0.5, 1, 0)

# Calculate train accuracies
probit_train_accuracy = sum(probit_train_predictions == train$class) / nrow(train)
identity_train_accuracy = sum(identity_train_predictions == train$class) / nrow(train)

# Predict on the test set using probit model
probit_test_predictions = predict(probit_model, newdata = test, type = "response")

# Predict on the test set using identity model
identity_test_predictions = predict(identity_model, newdata = test, type = "response")

# Convert probabilities to class predictions for the test set
probit_test_predictions = ifelse(probit_test_predictions >= 0.5, 1, 0)
identity_test_predictions = ifelse(identity_test_predictions >= 0.5, 1, 0)

# Calculate test accuracies
probit_test_accuracy = sum(probit_test_predictions == test$class) / nrow(test)
identity_test_accuracy = sum(identity_test_predictions == test$class) / nrow(test)

# Print the results
cat("Probit Model Training Accuracy:", probit_train_accuracy, "\n")
cat("Probit Model Test Accuracy:", probit_test_accuracy, "\n")
cat("Identity Model Training Accuracy:", identity_train_accuracy, "\n")
cat("Identity Model Test Accuracy:", identity_test_accuracy, "\n")

```

-   Based on these results, it appears that the identity model performs better than the probit model both in terms of training accuracy and test accuracy. However, the performance difference between the two models is relatively small.

```{r}
# Load the required packages
library(ROCR)

# Create prediction objects for the train set
probit_train_pred_obj = prediction(probit_train_predictions, train$class)
identity_train_pred_obj = prediction(identity_train_predictions, train$class)

# Calculate ROC-AUC for train set
probit_train_auc = as.numeric(performance(probit_train_pred_obj, "auc")@y.values)
identity_train_auc = as.numeric(performance(identity_train_pred_obj, "auc")@y.values)

# Create prediction objects for the test set
probit_test_pred_obj = prediction(probit_test_predictions, test$class)
identity_test_pred_obj = prediction(identity_test_predictions, test$class)

# Calculate ROC-AUC for test set
probit_test_auc = as.numeric(performance(probit_test_pred_obj, "auc")@y.values)
identity_test_auc = as.numeric(performance(identity_test_pred_obj, "auc")@y.values)

# Print the results
cat("Probit Model ROC-AUC (Train):", probit_train_auc, "\n")
cat("Probit Model ROC-AUC (Test):", probit_test_auc, "\n")
cat("Identity Model ROC-AUC (Train):", identity_train_auc, "\n")
cat("Identity Model ROC-AUC (Test):", identity_test_auc, "\n")

```

-   Based on the accuracy ROC-AUC and the consistent performance across training and test sets, the identity model appears to perform better than the probit model for this particular data.

### 10. Which model works better for this data?

### 11. If you have grouped data, use the methods for contingency tables to analyze the data (Chi sq test, G\^2, and so on if applicable).

```{r}
library(pROC)

# Predict classes using the logistic regression model
logistic_preds = predict(step.model, test, type = "response")
logistic_classes = ifelse(logistic_preds > 0.5, "p", "e")

# Create contingency table for logistic regression predictions
logistic_table = table(logistic_classes, test$class)

# Predict classes using the probit regression model
probit_preds = predict(probit_model, test, type = "response")
probit_classes = ifelse(probit_preds > 0.5, "p", "e")

# Create contingency table for probit regression predictions
probit_table = table(probit_classes, test$class)

# Predict classes using the Identity Links regression model
identity_preds = predict(identity_model, test)
identity_classes = ifelse(identity_preds > 0.5, "p", "e")

# Create contingency table for Identity Links regression predictions
identity_table = table(identity_classes, test$class)

# Print contingency table for logistic regression predictions
print(addmargins(logistic_table))

# Print contingency table for probit regression predictions
print(addmargins(probit_table))

# Print contingency table for Identity Links regression predictions
print(addmargins(identity_table))

```

```{r}
# Perform Chi-squared test for logistic regression predictions
chi2_logistic = chisq.test(logistic_table)
print(chi2_logistic)

# Perform Chi-squared test for probit regression predictions
chi2_probit = chisq.test(probit_table)
print(chi2_probit)

# Perform Chi-squared test for Identity Links regression predictions
chi2_identity = chisq.test(identity_table)
print(chi2_identity)
```

```{r}
fisher_logistic = fisher.test(logistic_table)
print(fisher_logistic)
# Perform Fisher's exact test for probit regression predictions
fisher_probit = fisher.test(probit_table)
print(fisher_probit)
# Perform Fisher's exact test for identity links regression predictions
fisher_identity = fisher.test(identity_table)
print(fisher_identity)
```

-   All three models (logistic regression, probit regression, and identity regression) demonstrate significant associations between the predicted classes and the actual classes. The tests indicate that the predicted classes are strongly associated with the actual classes, suggesting that these models have the potential to accurately predict the classes of the mushrooms based on the given features.

### 12. Write a report:

#### Introduction:

-   This report presents an analysis of mushroom classification using logistic regression, probit regression, and Identity Links regression models. The objective is to predict whether a mushroom is edible or poisonous based on various features.

#### Data Description:

-   The dataset used for the analysis contains 8,124 rows and 23 columns. Each row represents a mushroom, and the columns represent different attributes such as cap shape, cap surface, odor, gill color, and more. The "class" column indicates whether the mushroom is edible ("e") or poisonous ("p").

#### Logistic Regression Model:

-   A logistic regression model was fitted to the training data using all available features. The model yielded an accuracy of 96.35% on the test data and 96.96% on the train data. The area under the ROC curve (AUC) for the logistic regression model was 0.9408 on the test set.

#### Probit Regression Model:

-   A probit regression model was also fitted to the training data. The probit model achieved a test accuracy of 94.13% and a train accuracy of 94.42%. The AUC for the probit regression model was 0.9428 on the test set.\
    \#### Identity Links Regression Model:

-   In addition to logistic and probit regression, a Identity Links regression model was fitted to the training data. The Identity Links model achieved a test accuracy of 94.34% and a train accuracy of 94.43%. The AUC for the Identity Links regression model was 0.9436 on the test set.

#### Model Comparison:

-   Comparing the three models, logistic regression achieved the highest accuracy and AUC on the test set. However, all models performed relatively well in predicting the mushroom class, with accuracies above 94%. The differences in performance among the models were minimal.

#### Chi-Squared Test:

-   Chi-squared tests were conducted to assess the goodness of fit of each model's predictions to the actual classes. The p-values obtained for the logistic regression, probit regression, and Identity Links regression models were all less than 0.05, indicating a significant difference between the predicted and actual classes.

#### Fisher's Exact Test:

-   Fisher's exact tests were performed to examine the association between the predicted and actual classes. The p-values obtained for all three models were extremely small (p \< 0.001), suggesting a significant association between the predicted and actual classes.

#### Conclusion:

-   In conclusion, the logistic regression model demonstrated the highest accuracy and AUC in predicting the class of mushrooms as edible or poisonous. However, all three models showed good predictive performance. The chi-squared and Fisher's exact tests indicated a significant association between the predicted and actual classes, additionally validating the models' performance.
